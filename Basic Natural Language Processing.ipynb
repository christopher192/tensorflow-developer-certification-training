{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220feb39",
   "metadata": {},
   "source": [
    "## Natural Language Processing with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "12ddcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Library.helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8604142",
   "metadata": {},
   "source": [
    "### 1. Visualizing the NLP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e4179584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"Data/nlp_getting_started/train.csv\")\n",
    "test_df = pd.read_csv(\"Data/nlp_getting_started/test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0d7cd",
   "metadata": {},
   "source": [
    "let's shuffle the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3bbe8887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8b6fb",
   "metadata": {},
   "source": [
    "Check how many 0 (not disaster) and 1 (is disaster) in training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f35ed043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_shuffled.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d888eec",
   "metadata": {},
   "source": [
    "Since we have only two target values, therefore, we are dealing with <b>binary classification</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753fa0d",
   "metadata": {},
   "source": [
    "Let`s check how many total samples we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "09ed0ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 10876\n",
      "Total training samples: 7613, 70%\n",
      "Total test samples: 3263, 30%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")\n",
    "print(f\"Total training samples: {len(train_df)}, {round((len(train_df)/ (len(train_df) + len(test_df)) * 100))}%\")\n",
    "print(f\"Total test samples: {len(test_df)}, {round((len(test_df)/ (len(train_df) + len(test_df)) * 100))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ec511",
   "metadata": {},
   "source": [
    "Seem like we have decent amount of training and test dataset (70% vs 30%). Usually a split of 90/10 (90% training, 10% testing) or 80/20 is suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bee829",
   "metadata": {},
   "source": [
    "Let`s visualize the training examples to gain better understand on the dataset with randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8b0ae4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (is disaster)\n",
      "Text:\n",
      "Traffic Collision - No Injury: I5 S at I5 S 43rd Ave offramp South Sac http://t.co/cT9ejXoLpu\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (is disaster)\n",
      "Text:\n",
      "Eyewitness accounts of survivors of Hiroshima gleaned from a\n",
      "number of oral history projects https://t.co/yRQGNbLKaC\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (is disaster)\n",
      "Text:\n",
      "U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... http://t.co/SB5R7ShcCJ via @Change\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not disaster)\n",
      "Text:\n",
      "The chick I work with chews chewing gum so loud ?? feel to bang her\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not disaster)\n",
      "Text:\n",
      "@widda16 ... He's gone. You can relax. I thought the wife who wrecked her cake was a goner mind lol #whoops\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Create 0 and 7608, since we select 5 samples, we minus 5 so it will not exceed total number of samples.\n",
    "random_index = random.randint(0, len(train_df) - 5)\n",
    "\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index + 5].itertuples():\n",
    "    index, text, target = row\n",
    "    \n",
    "    print(f\"Target: {target}\", \"(is disaster)\" if target > 0 else \"(not disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc6fc8",
   "metadata": {},
   "source": [
    "### 2. Split training dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500abc7e",
   "metadata": {},
   "source": [
    "We`ll split the training dataset into 90% training vs 10% validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1541cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(), \n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(), \n",
    "                                                                            test_size=0.1, \n",
    "                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c8117",
   "metadata": {},
   "source": [
    "Let`s check the length after splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5d448d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca3c9c",
   "metadata": {},
   "source": [
    "Let`s view the top 10 first training sentences and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "65ab755a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4813e4",
   "metadata": {},
   "source": [
    "### 3. Converting the text into numbers (Tokenization and Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5c2a4",
   "metadata": {},
   "source": [
    "Let`s start text vectorization (tokenization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c28e9",
   "metadata": {},
   "source": [
    "Find the average number of words in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0adcad0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03753426",
   "metadata": {},
   "source": [
    "Create `TextVectorization` object and set some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c60c682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "759554ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer with available training text\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1604c3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[1246, 1246,   74,    9,  232,    4,    1,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let`s create sample sentence and tokenize it\n",
    "\n",
    "sample_sentence = \"Test test, there is flood in penang!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dcadaa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Tube strike = absolute pandemonium\n",
      "\n",
      "Vectorized version:\n",
      "[[1938 1251 6283  502    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "# Let`s choose some random sentence from the training dataset then tokenize it\n",
    "\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\n\\nVectorized version:\\n{text_vectorizer([random_sentence])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a3dcf153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# Let`s check the unique words in the vocabulary we have adapted\n",
    "\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70f517",
   "metadata": {},
   "source": [
    "### 4. Creating Embedding by using an Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "61096aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x2239b24dd60>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                             output_dim=128,\n",
    "                             embeddings_initializer=\"uniform\",\n",
    "                             input_length=max_length,\n",
    "                             name=\"embedding_1\") \n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c0ed6",
   "metadata": {},
   "source": [
    "Let`s get random sentence and test the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5ca8447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Hi yall this poem is called is the one about the snowstorm when we meet in space and that one time it rained. Thx. Ur watching disney chann\n",
      "\n",
      "Embedded version:\n",
      "[[[-0.03455232 -0.04213167  0.00866807 ...  0.01231498  0.00271455\n",
      "    0.04221163]\n",
      "  [ 0.03883579 -0.04218531  0.0122561  ... -0.04528418  0.03804381\n",
      "    0.02471515]\n",
      "  [ 0.02271518 -0.03204429  0.00166398 ... -0.00896945 -0.02778459\n",
      "   -0.04290668]\n",
      "  ...\n",
      "  [ 0.03493195  0.00647281 -0.00628376 ...  0.03327657  0.03445746\n",
      "    0.02336425]\n",
      "  [-0.01238489 -0.01569571  0.04614357 ...  0.00714378 -0.04799243\n",
      "   -0.04700608]\n",
      "  [ 0.02528647 -0.0374978   0.03416723 ... -0.02124472 -0.0327303\n",
      "   -0.02983623]]]\n"
     ]
    }
   ],
   "source": [
    "random_sentence = random.choice(train_sentences)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "\n",
    "print(f\"Original text:\\n{random_sentence}\\n\\nEmbedded version:\\n{sample_embed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e45f7d",
   "metadata": {},
   "source": [
    "Each token in the sentence gets turned into a length 128 feature vector. Let`s check out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1a839de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([-0.03455232, -0.04213167,  0.00866807, -0.01631409,  0.03413432,\n",
       "        0.02733302,  0.04049318, -0.02776693, -0.0237524 ,  0.02595422,\n",
       "        0.01845378,  0.00974258,  0.01647302,  0.00846023,  0.01980538,\n",
       "       -0.03098391,  0.02313794,  0.01934112, -0.04288423,  0.02086648,\n",
       "        0.01022495, -0.02217864,  0.03475136,  0.01083792, -0.00564416,\n",
       "       -0.01541098, -0.00291831, -0.0042215 , -0.0456175 , -0.03705328,\n",
       "        0.02297748,  0.04238257, -0.01124246,  0.01248912, -0.02637945,\n",
       "        0.0205338 ,  0.03695256,  0.01771487,  0.03142862,  0.02634046,\n",
       "       -0.0005298 , -0.03343501, -0.0301125 ,  0.02121196,  0.00588139,\n",
       "       -0.02367045,  0.04463694,  0.02741059, -0.01686595, -0.00685756,\n",
       "       -0.02562699,  0.0367169 , -0.04397279, -0.00650481, -0.00319433,\n",
       "        0.02268673,  0.03502214, -0.03258608,  0.04395398, -0.03344826,\n",
       "       -0.03235445, -0.03280754,  0.02930889, -0.0094143 , -0.04937797,\n",
       "        0.00325695, -0.01340131, -0.03136535,  0.03093759,  0.00373252,\n",
       "        0.0091782 , -0.02007406,  0.03967735,  0.0499577 , -0.00618235,\n",
       "       -0.02077411,  0.03741915, -0.04366698,  0.03591487, -0.01479018,\n",
       "       -0.00314542,  0.0363096 ,  0.03437134,  0.02214611, -0.04502422,\n",
       "        0.00031913, -0.00870739,  0.02964982, -0.00659096,  0.03016757,\n",
       "        0.02772829,  0.03035353,  0.01341884,  0.04276947,  0.01501948,\n",
       "       -0.02295239, -0.02695479,  0.02964283,  0.01017807,  0.01545621,\n",
       "        0.03608254,  0.01585065, -0.04829445, -0.03695866,  0.01450245,\n",
       "        0.00765695, -0.00831095,  0.04358413, -0.04225704,  0.04937586,\n",
       "        0.04092479, -0.04345785, -0.00272477, -0.04054781,  0.04958148,\n",
       "        0.01740083,  0.00471014, -0.00245447,  0.03545883, -0.00110934,\n",
       "       -0.03168271, -0.0103541 ,  0.03504015, -0.01145548, -0.03859323,\n",
       "        0.01231498,  0.00271455,  0.04221163], dtype=float32)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213df814",
   "metadata": {},
   "source": [
    "### 5. Modelling the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0dbe58",
   "metadata": {},
   "source": [
    "Let`s building the following:\n",
    "* **Model 1**: Naive Bayes (baseline)\n",
    "* **Model 2**: Feed-forward neural network (dense model)\n",
    "* **Model 3**: LSTM model\n",
    "* **Model 4**: GRU model\n",
    "* **Model 5**: Bidirectional-LSTM model\n",
    "* **Model 6**: 1D Convolutional Neural Network\n",
    "* **Model 7**: TensorFlow Hub Pretrained Feature Extractor\n",
    "* **Model 8**: model 7 with 10% of training data\n",
    "\n",
    "Model 1 is the baseline that we'll expect other models to beat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bde957",
   "metadata": {},
   "source": [
    "### Model 1: Naive Bayes (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "781f28aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model_1 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()),\n",
    "                    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "model_1.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8176b",
   "metadata": {},
   "source": [
    "Let`s checking accuracy of model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c99bb7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "baseline_score = model_1.score(val_sentences, val_labels)\n",
    "\n",
    "print(f\"The baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a2a27",
   "metadata": {},
   "source": [
    "Let make some predictions with baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9411d0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds = model_1.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f5f9c",
   "metadata": {},
   "source": [
    "### Creating an evaluation function for model experiments (Accuracy, Precision, Recall, F1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "480b4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a732db",
   "metadata": {},
   "source": [
    "Let`s evaluate model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0b9e5e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results = calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25658e3",
   "metadata": {},
   "source": [
    "### Model 2: Feed-forward neural network (dense model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "dcbd3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
    "x = text_vectorizer(inputs) # turn the input text into numbers\n",
    "x = embedding(x) # create an embedding of the numerized numbers\n",
    "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding, will increase accuracy and reduce loss\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
    "\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "8fd4c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "fed66bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_7 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "47cdf7c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 15ms/step - loss: 0.4245 - accuracy: 0.8745 - val_loss: 0.4675 - val_accuracy: 0.7927\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.2286 - accuracy: 0.9396 - val_loss: 0.4724 - val_accuracy: 0.7887\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.1771 - accuracy: 0.9475 - val_loss: 0.5013 - val_accuracy: 0.7835\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.1509 - accuracy: 0.9515 - val_loss: 0.5314 - val_accuracy: 0.7808\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.1335 - accuracy: 0.9566 - val_loss: 0.5629 - val_accuracy: 0.7795\n"
     ]
    }
   ],
   "source": [
    "model_2_history = model_2.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "857574b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 0.5629 - accuracy: 0.7795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5629136562347412, 0.7795275449752808]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the result.\n",
    "\n",
    "model_2.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d3e349aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_1/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n",
       " array([[-1.15883224e-01, -2.54726484e-02,  1.17380701e-01, ...,\n",
       "          5.77725209e-02,  1.40486538e-01,  1.75039843e-01],\n",
       "        [-1.88060123e-02, -4.11261916e-02,  2.04582456e-02, ...,\n",
       "          4.24626730e-02,  1.13175273e-01,  1.14244565e-01],\n",
       "        [-7.04165325e-02,  2.72392179e-04,  1.19271688e-01, ...,\n",
       "          2.51390859e-02,  4.85049635e-02,  1.14741907e-01],\n",
       "        ...,\n",
       "        [-3.30144390e-02, -5.24929911e-03, -4.20972481e-02, ...,\n",
       "          2.02876367e-02,  3.08806822e-03,  2.21579187e-02],\n",
       "        [-7.13161975e-02, -1.02247156e-01,  1.43731639e-01, ...,\n",
       "         -1.32509843e-01, -1.25343502e-01,  1.20486632e-01],\n",
       "        [-2.71486223e-01, -2.88029164e-01,  2.99228698e-01, ...,\n",
       "         -2.58708656e-01, -3.59207958e-01,  2.50938654e-01]], dtype=float32)>]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check embedding weights.\n",
    "\n",
    "embedding.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "74e7c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Check embedding weights.\n",
    "\n",
    "embed_weights = model_2.get_layer(\"embedding_1\").get_weights()[0]\n",
    "\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d087e2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3722633 ],\n",
       "       [0.7826859 ],\n",
       "       [0.99941486],\n",
       "       [0.06555247],\n",
       "       [0.01956049],\n",
       "       [0.9655756 ],\n",
       "       [0.9168829 ],\n",
       "       [0.9992175 ],\n",
       "       [0.99584955],\n",
       "       [0.35966796]], dtype=float32)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1ffb8d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn prediction probabilities into single-dimension tensor of floats\n",
    "\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs)) # squeeze removes single dimensions\n",
    "model_2_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4413cf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.95275590551181,\n",
       " 'precision': 0.7822644211580037,\n",
       " 'recall': 0.7795275590551181,\n",
       " 'f1': 0.7771404562571971}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_2 metrics\n",
    "\n",
    "model_2_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e76e52e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is the model 2 better than our baseline model?\n",
    "import numpy as np\n",
    "\n",
    "np.array(list(model_2_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a738d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 77.95, Difference: -1.31\n",
      "Baseline precision: 0.81, New precision: 0.78, Difference: -0.03\n",
      "Baseline recall: 0.79, New recall: 0.78, Difference: -0.01\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.01\n"
     ]
    }
   ],
   "source": [
    "# Compare both model 2 with baseline model.\n",
    "\n",
    "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
    "    for key, value in baseline_results.items():\n",
    "        print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")\n",
    "\n",
    "compare_baseline_to_new_results(baseline_results=baseline_results, \n",
    "                                new_model_results=model_2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ec9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
